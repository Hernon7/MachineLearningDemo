{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import findspark\n",
    "import pyspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext() #SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe out of it\n",
    "df = spark.read.parquet('data/hmp.parquet')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages for ETL process\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "\n",
    "minmaxscaler = $$\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,minmaxscaler])\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+\n",
      "|  x|  y|  z|              source|      class|\n",
      "+---+---+---+--------------------+-----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "+---+---+---+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages for ETL process\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+\n",
      "|  x|  y|  z|              source|      class|classIndex|\n",
      "+---+---+---+--------------------+-----------+----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "+---+---+---+--------------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "#the indexer is an estimater, it can remenber the value of string and need a transform function\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The SparkML can only deal with vector so we need to One-hot encoding\n",
    "# This process is a pure transformer\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the x,y,z into features\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "features_vectorized = vectorAssembler.transform(encoded)\n",
    "features_vectorized.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize features\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "features_normalized = normalizer.transform(features_vectorized)\n",
    "features_normalized.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The difference between a transformer and an estimator is state. A transformer is stateless whereas an estimator keeps state. Therefore “VectorAsselmbler” is a transformer since it only need to read row by row. Normalizer, on the other hand need to compute statistics on the dataset before, therefore it is an estimator. An estimator has an additional “fit” function. “OneHotEncoder” has been deprecated in Spark 2.3, therefore please change the code below to use the OneHotEstimator instead of the “OneHotEncoder”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline to speed up the feature engineering \n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "\n",
    "minmaxscaler = MinMaxScaler(inputCol=\"features_norm\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,minmaxscaler])\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.41244594513295846\n"
     ]
    }
   ],
   "source": [
    "#create a new pipeline to for the k-means\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\n",
    "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
    "model = pipeline.fit(df)\n",
    "predictions = model.transform(df)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6875664014387497 with k = 2\n",
      "Silhouette with squared euclidean distance = 0.6147915951361759 with k = 3\n",
      "Silhouette with squared euclidean distance = 0.6333227654128869 with k = 4\n",
      "Silhouette with squared euclidean distance = 0.5937447997439024 with k = 5\n",
      "Silhouette with squared euclidean distance = 0.592463658820136 with k = 6\n",
      "Silhouette with squared euclidean distance = 0.5484627422401509 with k = 7\n",
      "Silhouette with squared euclidean distance = 0.46686489256383346 with k = 8\n",
      "Silhouette with squared euclidean distance = 0.48034893889849645 with k = 9\n",
      "Silhouette with squared euclidean distance = 0.47370428136987536 with k = 10\n",
      "Silhouette with squared euclidean distance = 0.4819049717562352 with k = 11\n",
      "Silhouette with squared euclidean distance = 0.40964155503229643 with k = 12\n",
      "Silhouette with squared euclidean distance = 0.4153293521373778 with k = 13\n",
      "Silhouette with squared euclidean distance = 0.41244594513295846 with k = 14\n"
     ]
    }
   ],
   "source": [
    "# test K from 1 to 14\n",
    "k_list = list(range(2,15))\n",
    "for k in k_list:\n",
    "    kmeans = KMeans(featuresCol=\"features\").setK(k).setSeed(1)\n",
    "    pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
    "    model = pipeline.fit(df)\n",
    "    predictions = model.transform(df)\n",
    "\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print(\"Silhouette with squared euclidean distance = \" + str(silhouette),'with k =',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the normalized feature column and change the pipeline in order to contain the normalizer stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.41244594513295846\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(featuresCol=\"features_norm\").setK(14).setSeed(1)\n",
    "Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,kmeans])\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "predictions = model.transform(df)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, inflating the dataset helps, here we multiply x by 10, let’s see if the performance inceases.\n",
    "from pyspark.sql.functions import col\n",
    "df_denormalized = df.select([col('*'),(col('x')*10)]).drop('x').withColumnRenamed('(x * 10)','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.5709023393004293\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\n",
    "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
    "model = pipeline.fit(df_denormalized)\n",
    "predictions = model.transform(df_denormalized)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.15906267433367427\n"
     ]
    }
   ],
   "source": [
    "# Use GaussianMixture over KMeans\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture().setK(14).setSeed(1)\n",
    "pipeline = Pipeline(stages=[vectorAssembler, gmm])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "predictions = model.transform(df)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
